---
title: "R Notebook"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

```{r}
# Install
install.packages("tm")  # for text mining
install.packages("SnowballC") # for text stemming
install.packages("wordcloud") # word-cloud generator 
install.packages("RColorBrewer") # color palettes
install.packages("syuzhet") # for sentiment analysis
install.packages("ggplot2") # for plotting graphs
```


```{r}
# Load
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
library("syuzhet")
library("ggplot2")
```


```{r}
# Read the text file from local machine , choose file interactively
text <- readLines(file.choose())
```
# Load the data as a corpus
```{r}
TextDoc <- Corpus(VectorSource(text))
```
#Replacing "/", "@" and "|" with space
```{r}
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
TextDoc <- tm_map(TextDoc, toSpace, "/")
TextDoc <- tm_map(TextDoc, toSpace, "@")
TextDoc <- tm_map(TextDoc, toSpace, "\\|")
```
# Convert the text to lower case
```{r}
TextDoc <- tm_map(TextDoc, content_transformer(tolower))
```
# Remove numbers
```{r}
TextDoc <- tm_map(TextDoc, removeNumbers)
```
# Remove english common stopwords
```{r}
TextDoc <- tm_map(TextDoc, removeWords, stopwords("english"))
```
# Remove your own stop word
# specify your stopwords as a character vector
```{r}
TextDoc <- tm_map(TextDoc, removeWords, c("s", "company","team"))
```
# Remove punctuations
```{r}
TextDoc <- tm_map(TextDoc, removePunctuation)
```
# Eliminate extra white spaces
```{r}
TextDoc <- tm_map(TextDoc, stripWhitespace)
```
# Text stemming - which reduces words to their root form
```{r}
TextDoc <- tm_map(TextDoc, stemDocument)
```
# Build a term-document matrix
```{r}
TextDoc_dtm <- TermDocumentMatrix(TextDoc)
dtm_m <- as.matrix(TextDoc_dtm)
```
# Sort by descearing value of frequency
```{r}
dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
```
# Display the top 5 most frequent words
```{r}
head(dtm_d, 5)
```
```{r}
barplot(dtm_d[1:5,]$freq, las = 2, names.arg = dtm_d[1:5,]$word,
        col ="lightgreen", main ="Top 5 most frequent words",
        ylab = "Word frequencies")
```
#generate word cloud
```{r}
set.seed(1234)
wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 5,
          max.words=100, random.order=FALSE, rot.per=0.40, 
          colors=brewer.pal(8, "Dark2"))
```
# Word Association :

# Find associations 
```{r}
findAssocs(TextDoc_dtm, terms = c("good","work","health"), corlimit = 0.25)
```
# Find associations for words that occur at least 50 times
```{r}
findAssocs(TextDoc_dtm, terms = findFreqTerms(TextDoc_dtm, lowfreq = 50), corlimit = 0.25)

```
# possibly creat a heat map ?
# head(d,10) - to see top 10 lines of the get_nrc_sentiment dataframe
```{r}
head (TextDoc_dtm,10)
```
# regular sentiment score using get_sentiment() function and method of your choice
# please note that different methods have different scales
```{r}
syuzhet_vector <- get_sentiment(text, method="syuzhet")
```
# see the first 10 elements of the vector
```{r}
head(syuzhet_vector,10)
```
# see median value of vector elements
# median(syuzhet_vector)
```{r}
summary(syuzhet_vector)

```
#bing
```{r}
bing_vector <- get_sentiment(text, method="bing")
head(bing_vector)
summary(bing_vector)

```

#afinn
```{r}
#afinn
afinn_vector <- get_sentiment(text, method="afinn")
head(afinn_vector)
summary(afinn_vector)
```
#nrc
```{r}
nrc_vector <- get_sentiment(text, method="nrc")
head(nrc_vector)
median(nrc_vector)
```
#compare the first row of each vector using sign function
```{r}
rbind(
  sign(head(syuzhet_vector)),
  sign(head(bing_vector)),
  sign(head(afinn_vector))
)
```

# head(d,10) - just to see top 10 lines
```{r}
head (text,10)
```
# run nrc sentiment analysis to return data frame with each row classified as one of the following
# emotions, rather than a score : 
# anger, anticipation, disgust, fear, joy, sadness, surprise, trust 
# and if the sentiment is positive or negative

```{r}
d<-get_nrc_sentiment(text)
```
#transpose
```{r}
td<-data.frame(t(d))
```
#The function rowSums computes column sums across rows for each level of a grouping variable.
```{r}
td_new <- data.frame(rowSums(td[2:253]))
```
#Transformation and cleaning
```{r}
names(td_new)[1] <- "count"
td_new <- cbind("sentiment" = rownames(td_new), td_new)
rownames(td_new) <- NULL
td_new2<-td_new[1:8,]
```
#Plot 1 - count of words associated with each sentiment
```{r}
quickplot(sentiment, data=td_new2, weight=count, geom="bar",fill=sentiment,ylab="count")+ggtitle("Survey sentiments")

```
#Plot 2 - count of words associated with each sentiment, expressed as a percentage
```{r}
barplot(
  sort(colSums(prop.table(d[, 1:8]))), 
  horiz = TRUE, 
  cex.names = 0.7, 
  las = 1, 
  main = "Emotions in Text", xlab="Percentage"
)

```



Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.
